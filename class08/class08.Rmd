---
title: "Class 08"
author: "Gabriela Goldberg"
date: "4/26/2019"
output: html_document
---

## Kmeans clustering

Generate some example data for clustering

```{r}
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))

plot(x)
```

Use the kmeans() function setting k to 2 and nstart=20
Inspect/print the results

```{r}
rand_kmeans <- kmeans(x, 2, nstart = 20)
point_allocation <- rand_kmeans$cluster
cluster_size <- rand_kmeans$size
cluster_centers <- rand_kmeans$centers
```


>Q. How many points are in each cluster?
30 points in each cluster
    
>Q. What ‘component’ of your result object details
 - cluster size? SIZE
 - cluster assignment/membership? CLUSTER
 - cluster center? CENTER

Plot x colored by the kmeans cluster assignment and add cluster centers as blue points

```{r}
plot(x, type = "p", col = point_allocation, pch = 16)
points(cluster_centers, type = "p", col = "blue", pch = 8, cex = 2)
```

## Hierarchical clustering

Here we don't have to spell out "K" the number of clusters before-hand, but we do have to give it a distance matrix as input.

```{r}
d <- dist(x)
hc <- hclust(d)
hc
```

Let's plot the results

```{r}
plot(hc)
abline(h = 6, col = "red")
cutree(hc, h = 6)
```

Cut into two groups
```{r}
gp2 <- cutree(hc, k = 2)
```

Cut into three groups
```{r}
gp3 <- cutree(hc, k = 3)
```

Compare both clusterings
```{r}
table(gp2, gp3)
```

Our Turn!!!

Step 1. Generate some example data for clustering

```{r}
x <- rbind(
 matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
 matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
 matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
 rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
```

Step 2. Plot the data without clustering

```{r}
plot(x)
```

Step 3. Generate colors for known clusters
(just so we can compare to hclust results)

```{r}
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col, pch = 16)
```

>Q. Use the dist(), hclust(), plot() and cutree() functions to return 2 and 3 clusters 

```{r}
x_dist <- dist(x)
x_hc <- hclust(x_dist)
plot(x_hc)
x_2grp <- cutree(x_hc, k = 2)
x_3grp <- cutree(x_hc, k = 3)
```

>Q. How does this compare to your known 'col' groups?

```{r}
table(x_2grp)
table(x_3grp)
```

```{r}
table(x_2grp, x_3grp)
```

```{r}
plot(x, col = x_3grp, pch = 16)
```

```{r}
plot(x, col = x_2grp, pch = 16)
```

## Principal Component Analysis (PCA)

```{r}
mydata <- read.csv("https://tinyurl.com/expression-CSV",
 row.names=1) 
head(mydata)
```

There are `r nrow(mydata)` genes in this dataset.

Our genes are rows and our data are in columns.
  NOTE: prcomp() expects the samples to be rows and genes to be columns so we need to first transpose the matrix with the t() function!

```{r}
pca <- prcomp( t(mydata), scale. = TRUE)
summary(pca)
```

```{r}
attributes(pca)
```
  
Let's make our first PCA plot!

```{r}
plot(pca$x[,1], pca$x[,2])
```

Now we can determine the variation in the original data for each PC.

```{r}
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
```

```{r}
xlab <- paste("PC1 (", pca.var.per[1], "%)")
ylab <- paste("PC2 (", pca.var.per[2], "%)")
mycols <- c( rep("red", 5), rep("blue", 5))
plot(pca$x[,1], pca$x[,2], xlab = xlab, ylab = ylab, col = mycols, pch = 16)

```

## My turn!!

```{r}
x <- read.csv("UK_foods.csv")
```

>Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
nrow(x)
ncol(x)
head(x)
```

```{r}
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```

```{r}
dim(x)
```

```{r}
x <- read.csv("UK_foods.csv", row.names=1)
head(x)
```

>Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

The second option is preferred because it takes less lines of code.

Let's look at the data
```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

Q3: Changing what optional argument in the above barplot() function results in the following plot?
```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

>Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(10), pch=16)
```

>Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

The blue dot is different in this dataset for N. Ireland.

Let's do the PCA to see this difference!

```{r}
pca <- prcomp( t(x) )
summary(pca)
```

>Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

Plot PC1 vs PC2
```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500), pch = 16)
text(pca$x[,1], pca$x[,2], colnames(x))
```

>Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

```{r}
mycols <- c("orange", "red", "blue", "darkgreen")
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500), pch = 16, col = mycols)
text(pca$x[,1], pca$x[,2], colnames(x), col = mycols)
```


```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```

```{r}
z <- summary(pca)
z$importance
```

```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```

Lets focus on PC1 as it accounts for > 90% of variance 

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

